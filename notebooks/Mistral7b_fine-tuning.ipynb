{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e0e3e2",
   "metadata": {},
   "source": [
    "## Mistral-7B-Instruct fine tuning with PEFT/LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e283f",
   "metadata": {},
   "source": [
    "This notebook serves as a comprehensive guide for Mistral 7B Instruct fine-tuning with PEFT/LoRA on Amazon SageMaker. \n",
    "\n",
    "Refer to this [AWS Blog](https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/) post for more details. \n",
    "\n",
    "Steps: \n",
    "\n",
    "1. Prepare training data \n",
    "- Prepare the training dataset in csv format \n",
    "- Load the training data in Dataset\n",
    "- Process the data with Mistral model prompt template keys/tags\n",
    "\n",
    "2. Fine tuning\n",
    "- Load the Mistral-7B-Instruct-v0.2 model and tokenizer\n",
    "- Configure quantization if needed \n",
    "- Set PEFR/LoRA parameters\n",
    "- Define SFTTrainer with trainning args \n",
    "- Start the training process \n",
    "\n",
    "3. Save the trained model locally \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714279ab",
   "metadata": {},
   "source": [
    "### 0. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c044bf-5cc7-4154-84b1-1911ee2ae195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install datasets\n",
    "#!pip install py7zr\n",
    "#!pip install accelerate\n",
    "#!pip install bitsandbytes\n",
    "#!pip install peft\n",
    "#!pip install trl\n",
    "#!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6cc815-2012-427f-ae5d-4551671c9683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb64de6-aa77-469a-bd9d-66c47f3fac8b",
   "metadata": {},
   "source": [
    "### 1. Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77960030",
   "metadata": {},
   "source": [
    "Load the training data in Dataset. You need to put the data in a csv format with the following data fields  \n",
    "- instruction: question \n",
    "- response: reference answer used as ground truth for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc2e29-ffd9-47df-971f-06b56f5aebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a33b402-b7f5-4589-9b69-23f5162f44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE = \"data/your_train_dataset.csv\"\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=TRAINING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11854386",
   "metadata": {},
   "source": [
    "Add Mistral model keys/tags into the training data. The data used for training is in the following format   \n",
    "\n",
    "\\<s\\> [INST] your question [\\/INST] your reference answer \\<\\/s\\>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTION_KEY = \"<s>[INST]\"\n",
    "RESPONSE_KEY = \"[/INST]\"\n",
    "END_KEY = \"</s>\"\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "# This is a training prompt that does not contain an input string.  The instruction by itself has enough information\n",
    "# to respond.  For example, the instruction might ask for the year a historic figure was born.\n",
    "PROMPT_NO_INPUT_FORMAT = \"\"\"{instruction_key}{instruction}{response_key}{response}{end_key}\"\"\".format(\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    "    response=\"{response}\",\n",
    "    end_key=END_KEY,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb61627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_text(rec):\n",
    "        instruction = rec[\"instruction\"]\n",
    "        response = rec[\"response\"]\n",
    "        context = rec.get(\"context\")\n",
    "        \n",
    "        if not instruction:\n",
    "            raise ValueError(f\"Expected an instruction in: {rec}\")\n",
    "\n",
    "        if not response:\n",
    "            raise ValueError(f\"Expected a response in: {rec}\")\n",
    "\n",
    "        # For some instructions there is an input that goes along with the instruction, providing context for the\n",
    "        # instruction.  For example, the input might be a passage from Wikipedia and the instruction says to extract\n",
    "        # some piece of information from it.  The response is that information to extract.  In other cases there is\n",
    "        # no input.  For example, the instruction might be open QA such as asking what year some historic figure was\n",
    "        # born.\n",
    "        if context:\n",
    "            rec[\"text\"] = PROMPT_WITH_INPUT_FORMAT.format(instruction=(instruction.replace('\\n','')).replace('  ',''), \n",
    "                                                          response=(response.replace('\\n','')).replace('  ',''), \n",
    "                                                          input=(context.replace('\\n','')).replace('  ','')\n",
    "                                                         ).strip()\n",
    "        else:\n",
    "            rec[\"text\"] = PROMPT_NO_INPUT_FORMAT.format(instruction=(instruction.replace('\\n','')).replace('  ',''),\n",
    "                                                        response=(response.replace('\\n','')).replace('  ','')\n",
    "                                                       ).strip()\n",
    "        \n",
    "        return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(_add_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3681f3",
   "metadata": {},
   "source": [
    "Remove the original data fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a160182-6f42-4e4f-b06f-d758c77d4ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "        batched=True,\n",
    "        remove_columns=[\"instruction\", \"input\",\"response\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1310f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9777454-403a-41b1-9c45-3bfbaf0d855d",
   "metadata": {},
   "source": [
    "### 2. Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6cf7f0",
   "metadata": {},
   "source": [
    "Load the Mistral-7B-Instruct-v0.2 foundation model and configure the tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba182a1a-3819-4452-9dfa-cad2cceb40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "device_map=\"auto\"\n",
    "max_length = 2048  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ffc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to train a 4-bit quantized model (qLoRA), uncomment the below configuration\n",
    "\n",
    "#bnb_config = BitsAndBytesConfig(  \n",
    "#    load_in_4bit= True,\n",
    "#    bnb_4bit_quant_type= \"nf4\",\n",
    "#    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "#    bnb_4bit_use_double_quant= False,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ea8a2-3696-4281-ab46-27021b324cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(      \n",
    "    BASE_MODEL, \n",
    "    #quantization_config=bnb_config,        # uncomment if you want to train by qLoRA\n",
    "    torch_dtype=torch.bfloat16,             \n",
    "    device_map=device_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b877e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False # silence the warnings\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aee3e4-2ee1-4f0f-86d7-acb3c42bddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL, \n",
    ")\n",
    "\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc4157",
   "metadata": {},
   "source": [
    "Set PEFT/LoRA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57df3d9-81f0-47d1-80a2-0b7f6dbc6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    " r=256,                   \n",
    " lora_alpha=64,                        \n",
    " lora_dropout=0.05, \n",
    " target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\"], \n",
    " bias=\"none\",\n",
    " task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# add LoRA adaptor\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa93c0b-a7d7-4f95-9b77-2c5e6548ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'models/your_model_artifacts_dir'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e6b6dc",
   "metadata": {},
   "source": [
    "Setup trianing arguments and define SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d94157-d0cc-4bfa-ba7b-148de58e1ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" \n",
    "\n",
    "training_args = TrainingArguments(    \n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=10,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=20,    \n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=20000,\n",
    "        save_total_limit=10,\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        max_grad_norm=0.3,\n",
    "        max_steps=-1,\n",
    "        warmup_ratio=0.03,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        report_to=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840007b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length= None,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_args,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a3618c",
   "metadata": {},
   "source": [
    "Start the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ac5f7-d82c-4f12-b5be-9fb2c6620ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Training process time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8aa84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e67db2e4",
   "metadata": {},
   "source": [
    "### 3. Save the trained model to local storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de578e6b",
   "metadata": {},
   "source": [
    "After the training, you can save the model artifacts to local storage for futhre deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23894d8c-74dc-4de2-afe8-415580d39cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(OUTPUT_DIR)    \n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53c3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
