{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11061f1e",
   "metadata": {},
   "source": [
    "# Deploy Mistral-7B-instruct fine tuned model via DJL on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e4b1f",
   "metadata": {},
   "source": [
    "This notebook serves as a comprehensive guide for deploying Mistral 7B Instruct - LoRA fine-tuned on Amazon SageMaker using [DeepSpeed and DJL serving](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-tutorials-deepspeed-djl.html).   \n",
    "\n",
    "Refer to this [AWS Blog post](https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/) for more details. This model served as fine-tuned head of a custom RAG architecture, for more details check the blog post.\n",
    "\n",
    "Steps:\n",
    "1. **Prepare the Deployment Package**\n",
    "    * Organize the necessary files including requirements.txt, serving.properties, and model.py within a designated directory.\n",
    "    * Package the directory contents into a tar.gz file.\n",
    "    * Upload the Deployment Package to Amazon S3\n",
    "\n",
    "2. **Upload the packaged tar.gz file to an Amazon S3 bucket**\n",
    "    * Upload the packaged `tar.gz` file to an Amazon S3 bucket. This serves as the storage location for the deployment package.\n",
    "\n",
    "3. **Deploy the Model as a SageMaker Endpoint**\n",
    "    * Utilize SageMaker's capabilities to deploy the packaged model as an endpoint for later API inference.\n",
    "\n",
    "*Note: This notebook assumes familiarity with Amazon SageMaker, DJL, and basic concepts of deploying machine learning models. Additional documentation and resources are available for further reference and exploration.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6459f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75ebbab3",
   "metadata": {},
   "source": [
    "### 0. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93338149",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c244487f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee030be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "session = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = session._region_name\n",
    "\n",
    "image_uri = image_uris.retrieve(framework=\"djl-deepspeed\", version=\"0.24.0\", region=session._region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f356c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "788c5249",
   "metadata": {},
   "source": [
    "### 1. Preparing deployment package\n",
    "Our directory should have the following structure:\n",
    "\n",
    "your_local_dir    \n",
    "├── model.py    \n",
    "├── serving.properties    \n",
    "├── requirements.txt    \n",
    "└── fine-tuned model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced44b2-3222-4193-999e-a123c5c8c837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p faber_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8234b",
   "metadata": {},
   "source": [
    "Prepare requirements.txt and serving.properties in ./faber_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45413ead-eba0-4f9d-8fc7-c765168589d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile faber_lora/serving.properties\n",
    "engine=Python\n",
    "option.model_id=mistralai/Mistral-7B-Instruct-v0.2\n",
    "option.dtype=fp16\n",
    "option.tensor_parallel_degree=4\n",
    "option.enable_streaming=true\n",
    "option.entryPoint=model.py\n",
    "option.adapter_checkpoint=your_model_artifacts_dir\n",
    "option.adapter_name=your_adapter_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9df2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile your_local_dir/requirements.txt\n",
    "git+https://github.com/huggingface/transformers\n",
    "accelerate==0.23.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a53d6a",
   "metadata": {},
   "source": [
    "Prepare model.py in ./your_local_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834d798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile your_local_dir/model.py\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "from djl_python.inputs import Input\n",
    "from djl_python.outputs import Output\n",
    "from djl_python.encode_decode import encode, decode\n",
    "import torch\n",
    "\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from transformers import Pipeline, PreTrainedTokenizer\n",
    "\n",
    "device = \"cuda\"\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def Mistral_Infer(query,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        top_p=0.92,\n",
    "        top_k=0,\n",
    "        max_new_tokens=512,\n",
    "):\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    sequences = pipe(\n",
    "        f\"<s>[INST] {query} [/INST]\",\n",
    "        do_sample=do_sample,\n",
    "        max_new_tokens=max_new_tokens, \n",
    "        temperature=temperature, \n",
    "        top_k=top_k, \n",
    "        top_p=top_p,\n",
    "        num_return_sequences=1,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "   \n",
    "    full_text = sequences[0]['generated_text'].split('[STOP][STOP]')[0]\n",
    "    answer = full_text.split('[/INST]')[1]\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "def evaluate(instruction,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        top_p=0.92,\n",
    "        top_k=0,\n",
    "        max_new_tokens=512,\n",
    "        **kwargs,\n",
    "):\n",
    "    response = Mistral_Infer(instruction,\n",
    "                             do_sample,\n",
    "                             temperature,\n",
    "                             top_p,\n",
    "                             top_k,\n",
    "                             max_new_tokens\n",
    "                            )    \n",
    "    return response\n",
    " \n",
    "    \n",
    "def load_base_model(adapter_checkpoint, adapter_name):\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        return_dict=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(model, adapter_checkpoint, adapter_name)    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def inference(inputs: Input):\n",
    "    json_input = decode(inputs, \"application/json\")\n",
    "    sequence = json_input.get(\"inputs\")\n",
    "    generation_kwargs = json_input.get(\"parameters\", {})\n",
    "    output = Output()\n",
    "    outs = evaluate(sequence)\n",
    "    encode(output, outs, \"application/json\")\n",
    "    return output\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    \"\"\"\n",
    "    Default handler function\n",
    "    \"\"\"\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        # stateful model\n",
    "        props = inputs.get_properties()\n",
    "        model, tokenizer = load_base_model(props.get(\"adapter_checkpoint\"), props.get(\"adapter_name\"))\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # initialization request\n",
    "        return None\n",
    "\n",
    "    return inference(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f69307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff85cf7e",
   "metadata": {},
   "source": [
    "### 2. Upload model artifacts gz file to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc25d3",
   "metadata": {},
   "source": [
    "You can upload the files to S3 with AWS KMS key encryption, refer to [AWS documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a42a124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp -r your_model_artifacts_dir your_local_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b375e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "tar -cvzf your_model_package.tar.gz your_local_dir/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp your_model_package.tar.gz s3://your_s3_bucket/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75689d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e900557",
   "metadata": {},
   "source": [
    "### 3. Deploy as SageMaker Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9709f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select the EC2 instance type you prefer\n",
    "instance_type = \"ml.g5.2xlarge\"  \n",
    "\n",
    "model_s3_location = \"s3://your_s3_bucket/your_model_package.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90fb71a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker.djl_inference\n",
    "\n",
    "model = Model(\n",
    "    image_uri,\n",
    "    model_data=model_s3_location,\n",
    "    predictor_cls = sagemaker.djl_inference.DJLPredictor, \n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf3606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a9102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cf21567-3baa-4486-9f6a-fbafdf6d79ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Testing the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca22abfd-f8b1-4768-92b8-627224923d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "endpoint = 'the_deployed_sagemaker_endpoint_name'\n",
    "runtime = boto3.client('runtime.sagemaker')\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"ask your own question?\",\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5313e0c-9f7b-417f-92a3-82de0396c14a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint,\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=json.dumps(payload).encode(\"utf-8\"))\n",
    "\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e78719",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = json.loads(response['Body'].read())\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411bfb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9693f76f",
   "metadata": {},
   "source": [
    "### 5. Clean-up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462c95b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment the following lines to delete the endpoint and model\n",
    "# predictor.delete_endpoint()\n",
    "# model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c623440a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
